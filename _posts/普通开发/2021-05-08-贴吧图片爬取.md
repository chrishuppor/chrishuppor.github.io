---
layout: post
title: "贴吧图片爬取"
pubtime: 2021-05-08
updatetime: 2021-05-08
categories: Program
tags: Python
---

一个十分简陋的贴吧图片爬取程序，可以凑活一用，并不能全部爬取。

# 贴吧图片爬取

整体思路：先构造贴吧访问链接，然后获取每一页中帖子的链接，最后访问每个帖子获取图片链接并下载

```python
import re
import lxml
from urllib.parse import urlencode
from lxml import etree
import requests
from urllib.request import urlretrieve
import random
import time

#贴吧页面链接构造
def FormPageUrl(pagenum):
   title={
      'kw': '达瓦卓玛',#贴吧名称
      'ie': 'utf-8',
      'pn':'%d'%pagenum#贴吧页码，这里的页码是自然页码*50，从0开始
   }
   url='https://tieba.baidu.com/f?'+urlencode(title)
   return url

#获取网页的源码 
def GetHtmlText(url):
   header={'User-Agent':'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'} 
   html=requests.get(url,headers=header).text
   return html

#用正则表达式获取网页嵌有url的行
def GetInsertUrlLines(html):
   pattern='<a rel="noreferrer" href="(.*?)" title="'
   pattern=re.compile(pattern)
   article_urls=re.findall(pattern,html)
   return article_urls
 
#获取每篇文章的article_urls
def GetInsertUrls(article_urls):
   for i in range(len(article_urls)):
      if article_urls[i][1]=='/':
         article_urls[i]='https:'+article_urls[i] + "\n"
      else:
         article_urls[i]='https://tieba.baidu.com'+article_urls[i] + "\n"
   return (article_urls)
 
#修饰获得的具体每篇文章的链接，使其变成一个完整的网址，访问链接并下载html中出现的图片链接
def DownloadImage(md5dict, download_url, pn, header):
   try:
      if requests.get(download_url, headers=header, timeout=3).status_code == 200:#这里设置timeout属性很重要
         html = requests.get(download_url, headers=header, timeout=3).text#这里也要设置一下
         html = etree.HTML(html)
         imag_urls = html.xpath('//img[@class="BDE_Image"]/@src')#获取文章中的图片链接
         if len(imag_urls) != 0:
            for i in imag_urls:
               sleeptime = random.randint(2000, 5000) / 1000 #随机延时
               time.sleep(sleeptime)

               mdstr = i.split("/")[len(i.split("/")) - 1]#贴吧图片以md5命名，获取图片md5
               try:
                  md5dict[mdstr + "\n"] += 1
               except:
                  urlretrieve(i, r'C:\\Users\\TEST\\desktop\\test\\%s_%s%s'%(mdstr[:len(mdstr) - 4], pn, mdstr[len(mdstr) - 4:]))#urlretrieve()函数进行下载
                  md5dict[mdstr + "\n"] = 0
      else:
         print('[-]ERR GET ', download_url)
   except Exception as e:
      print("DownloadImage", e)

#把每个page的article链接存储到文档
def OutputArticleUrls(record_file):
   for page in range(0, 50 * 1010, 50):
      print("[+]page:", page // 50)
      if page // 50 % 285 == 0:#这里注意，频繁获取会被封，大概50min左右解封
         time.sleep(60 * 50)
         
      html=GetHtmlText(FormPageUrl(page))
      article_urls=GetInsertUrlLines(html)
      article_urls=GetInsertUrls(article_urls)
      print(len(article_urls))
      open(record_file, "a").writelines(article_urls)

#从记录文档中读取article链接
def GetArticleUrlsFromFiles(record_file):
   article_urls = open(record_file, "r").readlines()
   return article_urls

#------------------------------------------------------------------------
record_file = "c:\\users\\TEST\\desktop\\v.txt"
md5dict = dict()
OutputArticleUrls(record_file)

article_urls = GetArticleUrlsFromFiles(record_file)
article_urls = list(set(article_urls))

for url in article_urls:
   url = url.replace("\n", "")
   time.sleep(sleeptime)
   try:
      download_url = url
      DownloadImage(md5dict, download_url, download_url.split("/")[len(download_url.split("/")) - 1], header)
   except Exception as e:#异常抛出
      if e=="HTTPSConnectionPool(host='tieba.baidu.com', port=443): Read timed out. (read timeout=3)":
         download_url='http'+url[5:]#这个错误代表的是该是http开头的网址，写成了https,所有改一下网址重新下载
         DownloadImage(md5dict, download_url, download_url.split("/")[len(download_url.split("/")) - 1], header)
```


PS.又是不务正业的一天，但程序员不就应该这样，通过编程实现自己的小需求、方便自己的生活

