---
layout: post
title: "python爬虫之简易微博数据爬取"
pubtime: 2019-5-26
updatetime: 2019-5-26
categories: Program
tags: Python
---

一个简易的没有防爬虫检测机制的容易被挂掉的自用型微博数据爬取程序，主要用于爬取某个微博用户发表的感兴趣的微博文字——爬取奔跑吧嘉宾信息。

# python爬虫之简易微博数据爬取

## 数据分析

1. url分析

   1. 经网络搜索，发现自用爬虫大都使用手机微博的url。据网友说，手机微博相较于网页版，没有那么多限制，较容易爬取。因此，我也选择使用手机微博的网址——<https://m.weibo.cn/>

   2. 使用chrome访问手机微博，搜索“奔跑吧”，打开其微博页面。

   3. 打开chrome调试模式，刷新页面，可以看到如图信息。

      作为不懂WEB的人，自己是不会分析这些东西的。但通过学习他人的研究成果可知，下方两个红框的报文，分别是请求用户数据和用户第一页微博数据。

      ![图1 NetWork请求](https://chrishuppor.github.io/image/Snipaste_2019-05-26_20-03-57.PNG)

      两个请求的url分别为

      * https://m.weibo.cn/api/container/getIndex?uid=5242381821&luicode=10000011&lfid=100103type%3D1%26q%3D%E5%A5%94%E8%B7%91%E5%90%A7&sudaref=m.weibo.cn&type=uid&value=5242381821&containerid=1005055242381821

      * https://m.weibo.cn/api/container/getIndex?uid=5242381821&luicode=10000011&lfid=100103type%3D1%26q%3D%E5%A5%94%E8%B7%91%E5%90%A7&sudaref=m.weibo.cn&type=uid&value=5242381821&containerid=1076035242381821

      通过查看网友博客得知，如果要获取第二页数据，只需在uid=5242381821后面添加“&page=2”。

2. 返回数据分析

   1. 查看用户信息请求的返回数据

      如下，发现是json格式的。

      ![图2 response数据](https://chrishuppor.github.io/image/Snipaste_2019-05-26_20-13-16.PNG)

      使用[在线json解析](https://www.json.cn/)查看，如下，可以发现3个有意思的地方——微博总数、粉丝数、关注数。其中，我需要的是微博总数，这个数字控制着我的爬虫什么时候结束爬取*（PS.事实证明我想多了）*

      ![图3 解析结果](https://chrishuppor.github.io/image/Snipaste_2019-05-26_20-10-46.PNG)

   2. 查看用户微博数据请求的返回数据

      也是json格式的。使用[在线json解析](https://www.json.cn/)查看，如下，微博数据是以“card”的结构存放在card_list中的。

      ![图4 解析结果](https://chrishuppor.github.io/image/Snipaste_2019-05-26_20-21-00.PNG)

3. 至此，已经只要要访问的url和返回数据中目标数据的位置。接下来需要考虑感兴趣数据的提取了。

4. 文本提取分析

   1. 首先使用正则表达式去除微博文本中所有<>包含的数据，得到纯文本数据。

      如下，要去除其中被<>包含的所有字符

      ```
      <a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%A5%94%E8%B7%91%E5%90%A7%23&luicode=10000011&lfid=1076035242381821" data-hide=""><span class="surl-text">#奔跑吧#</span></a> 本期完整版视频在此～“情侣”搭档，爆笑连连；美食五子棋，拼的不是棋艺，是“嘴速”<span class="url-icon"><img alt=[偷笑] src="//h5.sinaimg.cn/m/emoticon/icon/default/d_touxiao-15afb1c739.png" style="width:1em; height:1em;" /></span>泥潭“大猪蹄子”守护战，顺便敷个纯天然“面膜”<span class="url-icon"><img alt=[笑而不语] src="//h5.sinaimg.cn/m/emoticon/icon/default/d_heiheihei-f7ca09d6e8.png" style="width:1em; height:1em;" /></span>高能推理撕名牌，全程考验智商，捂好“小心脏”<span class="url-icon"><img alt=[哈哈] src="//h5.sinaimg.cn/m/emoticon/icon/default/d_haha-dd1c6d36cf.png" style="width:1em; height:1em;" /></span>期待下期精彩节目～<a href='/n/李晨'>@李晨</a> <a href='/n/angelababy'>@angelababy</a> <a href='/n/郑恺'>@郑恺</a> <a href='/n/朱亚文'>@朱亚文</a> <a href='/n/王彦霖'>@王彦霖</a> <a href='/n/威神V_黄旭熙_LUCAS'>@威神V_黄旭熙_LUCAS</a> ...<a href="/status/4375805549081677">全文</a>
      ```

      得到

      ```
      #奔跑吧# 本期完整版视频在此～“情侣”搭档，爆笑连连；美食五子棋，拼的不是棋艺，是“嘴速”泥潭“大猪蹄子”守护战，顺便敷个纯天然“面膜”高能推理撕名牌，全程考验智商，捂好“小心脏”期待下期精彩节目～@李晨 @angelababy @郑恺 @朱亚文 @王彦霖 @威神V黄旭熙LUCAS ...全文
      ```

   2. 观察奔跑吧的微博规律——一般在微博最后@嘉宾，并且嘉宾顺序为先MC后本期嘉宾，且MC有一定的顺序。由此可见，@嘉宾的字符串是由确定特征的——以“@李晨 @angelababy @郑恺 @朱亚文 @王彦霖 @威神V_黄旭熙_LUCAS @宋雨琦_G-I-DLE”开头。因此提取这个字符串后面的数据就可以知道嘉宾是谁了。

## python脚本

```python
#!/usr/bin/python
# -*- coding: UTF-8 -*-

import requests
import json
import re
import sys
import imp

def GetResJsonDict(url):
    try:
        jsonStr = requests.get(url).content
        return json.loads(jsonStr)
    except:
        print("[-]a-o-,be detected.")
        return None


def GetVlogTotalCount(url_home):
    response_dict = GetResJsonDict(url_home)
    if None != response_dict:
        return response_dict["data"]["userInfo"]["statuses_count"]
    return 0


def ExtractGuest(full_str):
    plain_text = (re.sub("(<.*?>)", "", full_str))
    pattern = "@李晨 @angelababy @郑恺 @朱亚文 @王彦霖 @威神V_黄旭熙_LUCAS @宋雨琦_G-I-DLE "
    if pattern in plain_text and "浙江卫视" in plain_text:
        pos = plain_text.index(pattern)
        ret = plain_text[pos + len(pattern):]
        if len(ret) > 1:
            return ("[%s]"%ret)
    return None


def DealACard(card, tar_time):
    vlog_time = card["mblog"]["created_at"]
    if vlog_time not in tar_time:
        return None
        
    extra_mem = ExtractGuest(card["mblog"]["text"])
    if None == extra_mem:
        return None
    
    return "   time-%s extra members-%s"%(vlog_time, extra_mem)


def CrawlRunningExtraGuest(url_home, uid, tar_time):
    
    recent_count = 0; max_page = 100; mem_recorded = []
    
    print ("[*]start.")
                      
    total_vlog_count = GetVlogTotalCount(url_home)
    if total_vlog_count == 0:
        print ("[*]Game over!")
        return                      
    print ("[+]%d vlogs in total"%total_vlog_count)

    for page in range(1, max_page):
        url = 'https://m.weibo.cn/api/container/getIndex?uid=%d&page=%d&display=0&retcode=6102&type=uid&value=5242381821&containerid=1076035242381821'%(uid,page) #vlog_url
        response_dict = GetResJsonDict(url)
        if None == response_dict:
            break
            
        vlog_list = response_dict["data"]["cards"]
        for card in vlog_list:
            tmp = DealACard(card, tar_time)
            if tmp != None and tmp not in mem_recorded:
                print(tmp)
                mem_recorded.append(tmp)
                            
        recent_count += len(vlog_list)                                         
        print("[+]page-%d already read %d vblogs"%(page,recent_count))
        if recent_count > total_vlog_count:
            print("[*]Crawl over!")
            break

#-----------------------------------------------------------------------

imp.reload(sys)

url_home = "https://m.weibo.cn/api/container/getIndex?uid=5242381821&luicode=10000011&lfid=100103type%3D1%26q%3D%E5%A5%94%E8%B7%91%E5%90%A7&sudaref=m.weibo.cn&type=uid&value=5242381821&containerid=1005055242381821"
uid = 5242381821
tar_time = ["05-24","05-17", "05-10", "05-03", "04-26"]

CrawlRunningExtraGuest(url_home, uid, tar_time)
```

输出样例

![图5 输出结果样例](https://chrishuppor.github.io/image/Snipaste_2019-05-26_21-13-27.PNG)

# 后记

编写这个爬虫只是为了爬取“奔跑吧”发表的有关嘉宾信息的微博数据。因为我想知道奔跑吧最新一季目前每一期嘉宾都有谁，但在网上只搜到了MC信息没有搜到嘉宾信息，所以想去奔跑吧官微看嘉宾信息。果然，每一期播出当天，奔跑吧官微都会@本期嘉宾。但问题又来了——奔跑吧每天都织那么多围脖，肉眼找嘉宾好难。但“你以为我是谁呢”，程序员最擅长的就是支使计算机干活了(偷笑)。so，上吧，my dear pc！

这个爬虫十分简陋，没有任何防爬虫检测的功能，也没有根据返回数据自动生成url爬取数据功能，其实并不能承担起“爬虫”的称呼。而且最多爬取四十多页四百多条微博就会被挂掉，仅仅够用于我获取奔跑吧嘉宾名单(捂脸)。